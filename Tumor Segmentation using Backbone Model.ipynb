{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666},{"sourceId":2723611,"sourceType":"datasetVersion","datasetId":1659960},{"sourceId":9638426,"sourceType":"datasetVersion","datasetId":5885258},{"sourceId":9649374,"sourceType":"datasetVersion","datasetId":5893578},{"sourceId":9656364,"sourceType":"datasetVersion","datasetId":5898939},{"sourceId":9698877,"sourceType":"datasetVersion","datasetId":5923196},{"sourceId":9753073,"sourceType":"datasetVersion","datasetId":5971256}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n%pip install monai===0.7.0\n\n%pip install gdown==3.6.4\n%pip install segmentation_models_pytorch==0.2.0\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-11T06:43:54.259483Z","iopub.execute_input":"2024-11-11T06:43:54.259794Z","iopub.status.idle":"2024-11-11T06:44:39.137962Z","shell.execute_reply.started":"2024-11-11T06:43:54.259760Z","shell.execute_reply":"2024-11-11T06:44:39.136936Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting monai===0.7.0\n  Downloading monai-0.7.0-202109240007-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: torch>=1.5 in /opt/conda/lib/python3.10/site-packages (from monai===0.7.0) (2.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from monai===0.7.0) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai===0.7.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai===0.7.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai===0.7.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai===0.7.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai===0.7.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai===0.7.0) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.5->monai===0.7.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.5->monai===0.7.0) (1.3.0)\nDownloading monai-0.7.0-202109240007-py3-none-any.whl (650 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.2/650.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: monai\nSuccessfully installed monai-0.7.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting gdown==3.6.4\n  Downloading gdown-3.6.4.tar.gz (5.2 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from gdown==3.6.4) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown==3.6.4) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown==3.6.4) (4.66.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->gdown==3.6.4) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->gdown==3.6.4) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->gdown==3.6.4) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->gdown==3.6.4) (2024.8.30)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-3.6.4-py3-none-any.whl size=6109 sha256=019562342a774887a94efdded90aac785a826b1534e363bc7aceb1d2386511cb\n  Stored in directory: /root/.cache/pip/wheels/73/66/77/99342322fafc3a20e3a83cef3733f122d8a3d2d4be2fa61514\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-3.6.4\nNote: you may need to restart the kernel to use updated packages.\nCollecting segmentation_models_pytorch==0.2.0\n  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch==0.2.0) (0.19.0)\nCollecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch==0.2.0)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.6.3 (from segmentation_models_pytorch==0.2.0)\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting timm==0.4.12 (from segmentation_models_pytorch==0.2.0)\n  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (2.4.0)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.2.0)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.2.0) (4.66.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch==0.2.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch==0.2.0) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch==0.2.0) (1.3.0)\nDownloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12402 sha256=813b82521b5e6e6da0a5847c0e55dc513c6f624c5a9e68382c8f88a8b5b4d771\n  Stored in directory: /root/.cache/pip/wheels/61/3a/b0/0b4c443c380bd934701b0a25e4aed76479e4fcaf1a6f955664\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=993f72ac49ee1ca9b236ab01fa1b440ae4ccc4a9c46d3eb2ee811aed67cf5828\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.9\n    Uninstalling timm-1.0.9:\n      Successfully uninstalled timm-1.0.9\nSuccessfully installed efficientnet-pytorch-0.6.3 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.2.0 timm-0.4.12\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Instalize lib","metadata":{}},{"cell_type":"code","source":"# # import tarfile\n# # file = tarfile.open('../input/brats-2021-task1/BraTS2021_Training_Data.tar')\n\n# # file.extractall('./TrainingData')\n# # file.close()\n# import shutil\n# try:\n#     shutil.rmtree(\"/kaggle/working/TrainingData\")\n# except FileNotFoundError:\n#     print(\"The directory does not exist.\")\n# except OSError as e:\n#     print(f\"Error: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:41:02.567600Z","iopub.execute_input":"2024-10-29T10:41:02.568044Z","iopub.status.idle":"2024-10-29T10:41:02.572993Z","shell.execute_reply.started":"2024-10-29T10:41:02.567989Z","shell.execute_reply":"2024-10-29T10:41:02.571967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segementation","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\nimport logging\nimport pandas as pd \nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n\nfrom monai.data import DataLoader, ImageDataset\n\nfrom monai.transforms import (\n    AddChannel,\n    Compose,\n    Resize,\n    Transform\n)\n\n\nfrom sklearn.model_selection import GroupKFold\n\nimport torch.nn.functional as F\n\nfrom multiprocessing import Pool\n\n\nSETTINGS = {\n    \"DICOM_DATA_DIR\":\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification\",\n    \"TASK1_DIR\":\"/kaggle/working/TrainingData\", \n    \"CLASSIFICATION_RAW_JPG\":\"/kaggle/input/miccaibraintumorjpgdata\",\n    \"SEGMENT_DATA_DIR\":\"data/processed_segmentation_data\", \n    \"CLASSIFICATION_DATA_DIR\":\"data/processed_classification_data\",\n    \"KFOLD_PATH\":\"/kaggle/input/sub-file-for-tumor/data/train_stratifiedgroupkfold.csv\", \n    \"SEGMENT_MODEL_DIR\":\"/kaggle/working/models/densenet121_2d_segment\",\n    \"CLASSIFICATION_MODEL_DIR\":\"models/eca_nfnet_l0_2d_classification\",\n    \"TEMP_DATA_DIR\":\"temp\",\n    \"TEST_PREDICTION_FILE\":\"data/test_prediction.csv\"\n}\n\nIM_FOLDER_TASK1 = SETTINGS['TASK1_DIR']\n\nRUN_FOLDS = [0]\nKFOLD_PATH = SETTINGS['KFOLD_PATH']\n\nSEED = 67\nN_PROCESSES = 4\n\nOUT_FOLDER = SETTINGS['SEGMENT_DATA_DIR']\n\nPLANES = ['sagital', 'coronal', 'axial']\nMRI_TYPES = ['t1', 't1ce', 't2', 'flair']\n\nimport tarfile\n\n\n\n\n# ============ Helper functions ===========\nclass ScaleRange(Transform):\n    def __init__(self, new_max = 255.0):\n        super(ScaleRange, self).__init__()\n        self.new_max = new_max\n        \n    def __call__(self, data):\n        dmin, dmax = data.min(), data.max()\n        return (data - dmin) / (dmax-dmin) * self.new_max\n\nclass ConvertToMultiChannelBasedOnBratsClasses(Transform):\n    \"\"\"\n    Convert labels to multi channels based on brats classes:\n    label 2 is the peritumoral edema\n    label 4 is the GD-enhancing tumor\n    label 1 is the necrotic and non-enhancing tumor core\n    The possible classes are TC (Tumor core), WT (Whole tumor)\n    and ET (Enhancing tumor).\n    Ehancing Tumor (ET) = enhancing tumor\n    Tumor Core (TC) = enhancing tumor + necrotic\n    Whole Tumor (WT) = enhancing tumor + necrotic + edema    \n    \"\"\"\n\n    def __call__(self, masks):\n        '''This time we only use 2 label: 0 - WT and 1 - ET'''\n        result = []\n\n        # merge labels 1, 2 and 4 to construct WT\n        result.append(\n            np.logical_or(\n                np.logical_or(masks == 1, masks == 2), masks == 4\n            )\n        )\n        # label 4 is ET\n        result.append(masks == 4)\n        \n        return np.stack(result, axis=0).astype(np.float32)\n\ndef get_non_0_voxels_and_masks(voxels, masks_2channels, ax=0, min_avg=0.01):\n    '''Get non-empty slices from the 3D mask\n        A 2D slice is considered to be empty if its mean pixel value < min_avg'''\n    masks = np.logical_or(masks_2channels[0], masks_2channels[1])\n    remain_axes = tuple([i for i in range(len(voxels.shape)) if i != ax])\n    ax_mean = masks.mean(axis=remain_axes)\n    ax_non_0_inds = ax_mean > min_avg\n    if(ax==0):\n        return voxels[ax_non_0_inds], masks_2channels[:, ax_non_0_inds, :, :]\n    if(ax==1):\n        return voxels[:,ax_non_0_inds,:], masks_2channels[:, :, ax_non_0_inds,:]\n    if(ax==2):\n        return voxels[:,:,ax_non_0_inds], masks_2channels[:,:,:,ax_non_0_inds]\n    \ndef sampling_slices(non_0_voxels, non_0_masks, ax=0, keep_rate=0.1):\n    '''Nearby slices are similar to each other, we use sample to only get the different ones'''\n    total_slices = non_0_voxels.shape[ax]\n    T = max(round(total_slices * keep_rate), 1)\n    sampling_inds = np.arange(0, total_slices, T)\n    \n    if(ax==0):\n        return non_0_voxels[sampling_inds], non_0_masks[:, sampling_inds, :, :]\n    if(ax==1):\n        return non_0_voxels[:, sampling_inds, :], non_0_masks[:, :, sampling_inds, :]\n    if(ax==2):\n        return non_0_voxels[:, :, sampling_inds], non_0_masks[:, :, :, sampling_inds]\n    \n    \ndef process_one_patient(voxels, masks, patient_id):\n    '''Perform slicing 2D images and tumor masks for this patient'''\n    current_list_patient_id = []\n    current_list_plane = []\n    current_list_mri_type = []\n    current_list_slice_index = []\n    current_list_file_path = []\n    current_list_segfile_path = []\n    \n    for ax, plane in enumerate(PLANES):\n        non_0_voxels, non_0_masks = get_non_0_voxels_and_masks(voxels, masks, ax=ax)\n        if(non_0_voxels.shape[ax]==0):\n            print(f'Cannot get any slice in patient: {patient_id}, plane: {plane} due to the masks are too small')\n            continue\n        sampled_non_0_voxels, sampled_non_0_masks = sampling_slices(non_0_voxels, non_0_masks, ax=ax)\n\n        for j in range(sampled_non_0_voxels.shape[ax]):\n            file_path = os.path.join(OUT_FOLDER + '/2D_slice_data/', \n                                     f'BraTS2021_{patient_id:05d}',\n                                     f'BraTS2021_{patient_id:05d}_{mri_type}',\n                                    f'BraTS2021_{patient_id:05d}_{mri_type}_{plane}_{j:03d}')\n            seg_file_path = os.path.join(OUT_FOLDER + '/2D_slice_data/', \n                                     f'BraTS2021_{patient_id:05d}',\n                                    f'BraTS2021_{patient_id:05d}_segmask',\n                                    f'BraTS2021_{patient_id:05d}_segmask_{plane}_{j:03d}')\n\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            os.makedirs(os.path.dirname(seg_file_path), exist_ok=True)\n            \n            if(ax==0):\n                np.save(file_path, sampled_non_0_voxels[j])\n                np.save(seg_file_path, sampled_non_0_masks[:,j])\n            elif(ax==1):\n                np.save(file_path, sampled_non_0_voxels[:,j,:])\n                np.save(seg_file_path, sampled_non_0_masks[:,:,j,:])\n            elif(ax==2):\n                np.save(file_path, sampled_non_0_voxels[:,:,j])\n                np.save(seg_file_path, sampled_non_0_masks[:,:,:,j])\n            else:\n                raise ValueError('No such ax')\n\n            current_list_patient_id.append(patient_id)\n            current_list_plane.append(plane)\n            current_list_mri_type.append(mri_type)\n            current_list_slice_index.append(j)\n            current_list_file_path.append(file_path)\n            current_list_segfile_path.append(seg_file_path)\n\n    return current_list_patient_id, current_list_plane, current_list_mri_type,  \\\n            current_list_slice_index, current_list_file_path, current_list_segfile_path\n\n\ndef update(args):\n    global list_patient_id, list_plane, list_mri_type, list_slice_index, list_file_path, list_segfile_path\n    pbar.update()\n    current_list_patient_id, current_list_plane, current_list_mri_type,  \\\n            current_list_slice_index, current_list_file_path, current_list_segfile_path = args\n    \n    list_patient_id += current_list_patient_id\n    list_plane += current_list_plane\n    list_mri_type += current_list_mri_type\n    list_slice_index += current_list_slice_index\n    list_file_path += current_list_file_path\n    list_segfile_path += current_list_segfile_path\n\n\ndef error(e):\n    print(e)\n        \n# =========================================\n\n# ============ Read meta data =============\nfold_df = pd.read_csv(KFOLD_PATH)\nfold_df['pfolder'] = fold_df.BraTS21ID.map(lambda x: f'BraTS2021_{x:05d}')\n\nPATIENT_DIRS = []\nfor p in os.listdir(IM_FOLDER_TASK1):\n    try:\n       \n        int(p.split('_')[-1])\n        PATIENT_DIRS.append(p)\n    except:\n        print('Non patient dir:', p)\n\ndf = pd.DataFrame(PATIENT_DIRS, columns=['pfolder'])\n\n\ndf['BraTS21ID'] = df['pfolder'].map(lambda x: int(x.split('_')[-1]))\ndf = df.dropna()\n\ndf = df[~df.BraTS21ID.isin(fold_df.BraTS21ID.tolist())]\n\nfor t in MRI_TYPES:\n    df[f'{t}_data_path'] = df.pfolder.map(lambda x: os.path.join(IM_FOLDER_TASK1, x, x+f'_{t}.nii.gz'))\ndf['seg_label_path'] = df.pfolder.map(lambda x: os.path.join(IM_FOLDER_TASK1, x, x+f'_seg.nii.gz'))\n\n# =========================================\n\n\n# ============ Create a nii gz file loader ==========\ntransforms = Compose([ScaleRange()])\n\nseg_transforms = Compose([ConvertToMultiChannelBasedOnBratsClasses(),\n                         ])\n\nmri_type = MRI_TYPES[0]\n# Define nifti dataset, data loader\ndataset = ImageDataset(image_files=df[f'{mri_type}_data_path'].tolist(),\n                             seg_files = df.seg_label_path.tolist(),\n                             seg_transform=seg_transforms,\n                            transform=transforms\n                      )\n# =====================================================\n\n\n\n# ========== Perform slicing data and mask ============\n\nfor mri_type in MRI_TYPES:\n    dataset = ImageDataset(image_files=df[f'{mri_type}_data_path'].tolist(),\n                                 seg_files = df.seg_label_path.tolist(),\n                                   labels = df['BraTS21ID'].tolist(),\n                                 seg_transform=seg_transforms,\n                                transform=transforms\n                          )\n    \n    os.makedirs(OUT_FOLDER + '/2D_slice_data/', exist_ok=True)\n\n    list_patient_id = []\n    list_plane = []\n    list_mri_type = []\n    list_slice_index = []\n    list_file_path = []\n    list_segfile_path = []\n\n    pool = Pool(processes=N_PROCESSES)   \n\n    iterations = range(len(dataset))\n    pbar = tqdm(iterations)\n\n    for i in iterations:\n        voxels, masks, patient_id = dataset[i]\n        pool.apply_async(\n            process_one_patient,\n            args=(voxels, masks, patient_id),\n            callback=update,\n            error_callback=error,\n        )\n\n    pool.close()\n    pool.join()\n    pbar.close()\n    \nout_df = pd.DataFrame({\n    'BraTS21ID':list_patient_id,\n    'mri_type':list_mri_type,\n    'plane':list_plane,\n    'slice_index':list_slice_index,\n    'file_path':list_file_path,\n    'segfile_path':list_segfile_path\n})\n\nout_df.to_csv(os.path.join(OUT_FOLDER, 'segment_meta.csv'))\n\n\n\n  ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-29T10:41:02.574296Z","iopub.execute_input":"2024-10-29T10:41:02.574608Z","iopub.status.idle":"2024-10-29T10:41:02.592468Z","shell.execute_reply.started":"2024-10-29T10:41:02.574576Z","shell.execute_reply":"2024-10-29T10:41:02.591561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimg=np.load(\"/kaggle/input/processed-segementation/processed_segmentation_data/2D_slice_data/BraTS2021_00000/BraTS2021_00000_segmask/BraTS2021_00000_segmask_axial_000.npy\")\nmask=np.load(\"/kaggle/input/processed-segementation/processed_segmentation_data/2D_slice_data/BraTS2021_00000/BraTS2021_00000_t1/BraTS2021_00000_t1_axial_000.npy\")\nprint(img.shape)\nprint(mask.shape)\n\nplt.figure()\nplt.imshow(mask)\nplt.imshow(img[1,:,:], cmap=\"gray\", alpha=0.5)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:41:02.594656Z","iopub.execute_input":"2024-10-29T10:41:02.594978Z","iopub.status.idle":"2024-10-29T10:41:02.606386Z","shell.execute_reply.started":"2024-10-29T10:41:02.594946Z","shell.execute_reply":"2024-10-29T10:41:02.605387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"\nos.makedirs('models/densenet121_2d_segment', exist_ok=True)\nurl = 'https://drive.google.com/uc?id=12EVeyHI_kQlryAp6554Au4S1pt1ektnY'\noutput = 'models/densenet121_2d_segment/Fold0_densenet121_2d_segment.pth'\ngdown.download(url, output, quiet=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:41:02.607488Z","iopub.execute_input":"2024-10-29T10:41:02.607823Z","iopub.status.idle":"2024-10-29T10:41:02.616828Z","shell.execute_reply.started":"2024-10-29T10:41:02.607792Z","shell.execute_reply":"2024-10-29T10:41:02.615900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport torch\nimport random\nimport sys\nimport os\nimport matplotlib.pyplot as plt\nimport logging\n\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom typing import List\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef get_logger(name, path, mode='a'):\n    logger = logging.getLogger(name)  \n\n    if not logger.hasHandlers():\n        # set log level\n        logger.setLevel(logging.INFO)\n\n        # define file handler and set formatter\n        file_handler = logging.FileHandler(path, mode=mode)\n        formatter    = logging.Formatter('%(asctime)s : %(levelname)s : %(name)s : %(message)s')\n        file_handler.setFormatter(formatter)\n\n        # add file handler to logger\n        logger.addHandler(file_handler)\n    \n    return logger\n    \n    \ndef log_and_print(logger, obj):\n    print(obj)\n    logger.info(obj)    \n    \ndef init_progress_dict(metrics):\n    progress_dict = dict()\n    for metric in metrics:\n        progress_dict[f'train_{metric}'] = []\n        progress_dict[f'valid_{metric}'] = []\n    return progress_dict\n\ndef log_to_progress_dict(progress_dict, metric_dict):\n    for k, v in metric_dict.items():\n        progress_dict[k].append(v)\n       \n    return progress_dict\n\ndef save_progress(progress_dict, out_folder, out_folder_name, fold, show=False):\n    metric_names = list(progress_dict.keys())\n    epochs = len(progress_dict[metric_names[0]])+1\n    \n    # plot figure and save the progress chart\n    n_cols = 4\n    n_rows = int(np.ceil(len(metric_names) / 2 / n_cols))\n    \n    plt.figure(figsize=(7*n_cols, 7*n_rows))\n    \n    for i in range(0, len(metric_names), 2):\n        plt.subplot(n_rows,n_cols,int(i/2+1))\n\n        plt.plot(range(1, epochs), progress_dict[metric_names[i]])\n        plt.plot(range(1, epochs), progress_dict[metric_names[i+1]])\n        plt.legend([metric_names[i], metric_names[i+1]])\n        plt.xlabel('Epoch')\n        plt.title(f'{metric_names[i]} and {metric_names[i+1]}')\n\n    save_name = f'training_progress_{out_folder_name}_fold{fold}'\n    plt.savefig(os.path.join(out_folder, save_name+'.jpg'))\n\n    if(show):\n        plt.show()\n\n    pd.DataFrame({'epoch':range(1, epochs), **progress_dict}).to_csv(os.path.join(out_folder, save_name+'.csv'), index=False)\n\n    \ndef check_mem(cuda_device):\n    devices_info = os.popen('\"/usr/bin/nvidia-smi\" --query-gpu=memory.total,memory.used --format=csv,nounits,noheader').read().strip().split(\"\\n\")\n    total, used = devices_info[int(cuda_device)].split(',')\n    print(used)\n    return total,used\n\ndef occumpy_mem(cuda_device):\n    total, used = check_mem(cuda_device)\n    total = int(total)\n    used = int(used)\n    max_mem = int(total * 0.9)\n    block_mem = max_mem - used\n    x = torch.cuda.FloatTensor(256,1024,block_mem)\n    del x","metadata":{"execution":{"iopub.status.busy":"2024-11-05T04:53:36.160570Z","iopub.execute_input":"2024-11-05T04:53:36.161171Z","iopub.status.idle":"2024-11-05T04:53:39.383274Z","shell.execute_reply.started":"2024-11-05T04:53:36.161122Z","shell.execute_reply":"2024-11-05T04:53:39.382447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport logging\nimport pandas as pd \nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\n\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CosineAnnealingLR\n\nimport torch.nn.functional as F\n\nimport json\nimport gc\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nfrom segmentation_models_pytorch.unetplusplus.model import UnetPlusPlus\nfrom segmentation_models_pytorch.losses import DiceLoss\nfrom segmentation_models_pytorch.utils.metrics import IoU\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport argparse\n\n# parser = argparse.ArgumentParser(description='Insert some arguments')\n# parser.add_argument('--gpu', type=int,\n#                     help='GPU ID', default=0)\n# parser.add_argument('--batch_size', type=int,\n#                     help='Batch size', default=128)\n# parser.add_argument('--n_workers', type=int,\n#                     help='Number of parrallel workers', default=8)\n# args = parser.parse_args()\nargv = sys.argv[1:]  # Exclude the first argument (script name)\nif '-f' in argv:\n    argv = argv[:argv.index('-f')]  # Ignore everything after '-f'\n\n# Set up the parser\nparser = argparse.ArgumentParser(description='Insert some arguments')\nparser.add_argument('--gpu', type=int, help='GPU ID', default=0)\nparser.add_argument('--batch_size', type=int, help='Batch size', default=128)\nparser.add_argument('--n_workers', type=int, help='Number of parallel workers', default=8)\n\n# Parse the cleaned arguments\nargs = parser.parse_args(argv)\n\n\n# with open('SETTINGS.json', 'r') as f:\nSETTINGS = {\n    \"DICOM_DATA_DIR\":\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification\",\n    \"TASK1_DIR\":\"/kaggle/working/TrainingData\", \n    \"CLASSIFICATION_RAW_JPG\":\"/kaggle/input/miccaibraintumorjpgdata\",\n    \"SEGMENT_DATA_DIR\":\"/kaggle/input/segmentaed-csv\", \n    \"CLASSIFICATION_DATA_DIR\":\"data/processed_classification_data\",\n    \"KFOLD_PATH\":\"/kaggle/input/sub-file-for-tumor/data/train_stratifiedgroupkfold.csv\", \n    \"SEGMENT_MODEL_DIR\":\"/kaggle/working/models/densenet121_2d_segment\",\n    \"CLASSIFICATION_MODEL_DIR\":\"models/eca_nfnet_l0_2d_classification\",\n    \"TEMP_DATA_DIR\":\"temp\",\n    \"TEST_PREDICTION_FILE\":\"data/test_prediction.csv\"\n        }\n\nFOLDER = SETTINGS['SEGMENT_DATA_DIR']\nMETA_FILE_PATH = os.path.join(SETTINGS['SEGMENT_DATA_DIR'], 'segment_meta_groupkfold.csv')\n\nRUN_FOLDS = [0]\nSEED = 67\nDIM = (128,128, 3)\n# N_WORKERS = args.n_workers\nBATCH_SIZE = 5\nBASE_LR = 1e-2\nNUM_EPOCHS = 100\nPATIENT = 10\nSAMPLE = None\nDEVICE = torch.device(f'cuda:{args.gpu}')\n\nPARENT_OUT_FOLDER = f'models/'    \n\nCANDIDATES = [\n    {\n        'backbone_name':'densenet121',\n        'ver_note':'2d_segment',\n        'backbone_pretrained':None,\n        'batch_size':BATCH_SIZE,\n        'warm_up_epochs':5,\n    },\n]\n\nimport sys\n# from utils.general import seed_torch, init_progress_dict, log_to_progress_dict, save_progress, log_and_print, get_logger\n\n# seed every thing\nseed_torch(SEED)\n\n# ================= Some helper functions ====================\nclass BrainSegment2DDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, csv, transforms=None):\n        self.csv = csv.reset_index(drop=True)\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        image = np.load(row['file_path']+'.npy')\n        image = np.stack([image]*3, axis=-1)\n        mask = np.load(row['segfile_path']+'.npy')\n        mask = np.stack([mask[0], mask[1]], axis=-1)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            mask = mask.permute(2,0,1)\n        \n        return image, mask\n    \n\ndef get_train_transforms(candidate):\n    dim = candidate.get('dim', DIM)\n    return A.Compose(\n        [\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(p=0.5),\n     \n            A.Resize(width=dim[1], height=dim[0], always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_valid_transforms(candidate):\n    dim = candidate.get('dim', DIM)\n    return A.Compose(\n        [\n            A.Resize(width=dim[1], height=dim[0], always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_model(candidate):\n    model = UnetPlusPlus(\n        encoder_name = candidate['backbone_name'],\n        encoder_depth = 5,\n        encoder_weights = None,\n        classes = 2,\n        activation = 'sigmoid',\n    )\n\n    weight_path = candidate.get('backbone_pretrained')\n    if(weight_path is not None):\n        print('Load pretrained:', weight_path)\n        model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n        \n    return model\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef train_valid_fn(dataloader,model,criterion,iou_metric,optimizer=None,device='cuda:0',\n                            scheduler=None,epoch=0, mode='train', scaler=None):\n    '''Perform model training'''\n    if(mode=='train'):\n        model.train()\n    elif(mode=='valid'):\n        model.eval()\n    else:\n        raise ValueError('No such mode')\n        \n    loss_score = AverageMeter()\n    iou_score = AverageMeter()\n    \n    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n    for i, batch in tk0:\n        if(mode=='train'):\n            optimizer.zero_grad()\n            \n        # input, gt\n        images, gt_masks = batch\n        images = images.to(DEVICE)\n        gt_masks = gt_masks.to(DEVICE)\n\n        with torch.cuda.amp.autocast():\n            # prediction\n            pred_masks = model(images)\n\n            # compute loss\n            loss = criterion(y_true=gt_masks, y_pred=pred_masks)\n            \n            # compute metric\n            iou = iou_metric(pred_masks, gt_masks)\n        \n        if(mode=='train'):\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        \n        loss_score.update(loss.detach().cpu().item(), dataloader.batch_size)\n        iou_score.update(iou.detach().cpu().item(), dataloader.batch_size)\n        \n        if(mode=='train'):\n            tk0.set_postfix(Loss_Train=loss_score.avg, IOU_Train=iou_score.avg, \n                            Epoch=epoch, LR=optimizer.param_groups[0]['lr'])\n        elif(mode=='valid'):\n            tk0.set_postfix(Loss_Valid=loss_score.avg, IOU_Valid=iou_score.avg, Epoch=epoch)\n        \n        del batch, images, gt_masks, pred_masks, loss, iou\n        torch.cuda.empty_cache()\n        \n    if(mode=='train'):\n        if(scheduler.__class__.__name__ == 'CosineAnnealingWarmRestarts'):\n            scheduler.step(epoch=epoch)\n        elif(scheduler.__class__.__name__ == 'ReduceLROnPlateau'):\n            scheduler.step(loss_score.avg)\n    \n    return loss_score.avg, iou_score.avg\n\ndef dfs_freeze(module):\n    for name, child in module.named_children():\n        for param in child.parameters():\n            param.requires_grad = False\n        dfs_freeze(child)\n        \ndef dfs_unfreeze(module):\n    for name, child in module.named_children():\n        for param in child.parameters():\n            param.requires_grad = True\n        dfs_unfreeze(child)\n# ===========================================================\n  \n# ===========================================================\n        \n    \n# ================ Read metadata =================\ndf = pd.read_csv(META_FILE_PATH)\n# ================================================\n\n\n# ============================ Training ==============================\nfor candidate in CANDIDATES:\n    print(f\"######################### Candidate: {candidate['backbone_name']} ############################\")\n    run_folds = candidate.get('run_folds', RUN_FOLDS)\n    \n    parent_out_folder = candidate.get('parent_out_folder', PARENT_OUT_FOLDER)\n    ver_note = candidate['ver_note']\n    out_folder_name = f\"{candidate['backbone_name']}_{ver_note}\"\n    out_folder = os.path.join(parent_out_folder, out_folder_name)\n\n    os.makedirs(out_folder, exist_ok=True)\n    \n    for valid_fold in run_folds:\n        # Read data\n        if(SAMPLE):\n            df = df.sample(SAMPLE, random_state=SEED)\n\n        train_df = df[df.fold!=valid_fold]\n        valid_df = df[df.fold==valid_fold]\n\n        print(f'\\n\\n================= Fold {valid_fold} ==================')\n        print(f'Number of training images: {len(train_df)}. Number of valid images: {len(valid_df)}')\n        print(\"filepath\",len(valid_df))\n        \n        train_dataset = BrainSegment2DDataset(train_df, get_train_transforms(candidate))\n        valid_dataset = BrainSegment2DDataset(valid_df, get_valid_transforms(candidate))\n        \n        batch_size = candidate.get('batch_size', BATCH_SIZE)\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n        \n        # model\n        model = get_model(candidate)\n        # freeze layer\n        dfs_freeze(model.encoder)\n        print(' -------- Start warm up process ----------')\n        print('Freeze encoder')\n        model.to(DEVICE)\n        print()\n        \n        # Optimizer and scheduler\n        base_lr = candidate.get('base_lr', BASE_LR)\n        optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=BASE_LR)\n\n        num_training_steps = NUM_EPOCHS * len(train_loader)\n        lr_scheduler = ReduceLROnPlateau(optimizer=optim)\n\n        # loss\n        criterion = DiceLoss(mode='binary', from_logits=False)\n        iou_metric = IoU()\n\n        # use amp to accelerate training\n        scaler = torch.cuda.amp.GradScaler()\n\n        # Logging\n        logger = get_logger(\n            name = f'training_log_fold{valid_fold}.txt',\n            path=os.path.join(out_folder, f'training_log_fold{valid_fold}.txt')\n        )\n\n        best_valid_loss = 9999\n        best_valid_ep = 0\n        patient = PATIENT\n\n        progress_dict = init_progress_dict(['loss', 'IOU'])\n\n        start_ep = candidate.get('warm_start_ep', 1)\n        print('Start ep:', start_ep)\n\n        # warm up epochs\n        warm_up_epochs = candidate.get('warm_up_epochs', 1)\n\n        \n        for epoch in range(start_ep, NUM_EPOCHS+1):\n            if (epoch==warm_up_epochs+1):\n                print(' -------- Finish warm up process ----------')\n                print('Unfreeze encoder')\n                dfs_unfreeze(model.encoder)\n                optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=BASE_LR)\n                lr_scheduler = ReduceLROnPlateau(optimizer=optim)\n                \n            # =============== Training ==============\n            train_loss, train_iou = train_valid_fn(train_loader,model, criterion, iou_metric,\n                                                    optimizer=optim,device=DEVICE,\n                                                    scheduler=lr_scheduler,epoch=epoch,mode='train',\n                                                  scaler=scaler)\n            valid_loss, valid_iou = train_valid_fn(valid_loader,model, criterion, iou_metric,\n                                                     device=DEVICE, \n                                                     epoch=epoch,mode='valid',\n                                                  scaler=scaler)\n\n            current_lr = optim.param_groups[0]['lr']\n            log_line = f'Model: {out_folder_name}. Epoch: {epoch}. '\n            log_line += f'Train loss:{train_loss} - Valid loss: {valid_loss}. '\n            log_line += f'Train IOU:{train_iou} - Valid IOU: {valid_iou}. '\n            log_line += f'Lr: {current_lr}.'\n\n            log_and_print(logger, log_line)\n\n            metric_dict = {'train_loss':train_loss,'valid_loss':valid_loss,\n                           'train_IOU':train_iou, 'valid_IOU':valid_iou,\n                       }\n\n            progress_dict = log_to_progress_dict(progress_dict, metric_dict)\n\n            # plot figure and save the progress chart\n            save_progress(progress_dict, out_folder, out_folder_name, valid_fold, show=False)\n\n            if(valid_loss < best_valid_loss):\n                best_valid_loss = valid_loss\n                best_valid_ep = epoch\n                patient = PATIENT # reset patient\n\n                # save model\n                name = os.path.join(out_folder, 'Fold%d_%s.pth'%(valid_fold, \n                                                                 out_folder_name, \n                                                                ))\n                log_and_print(logger, 'Saving model to: ' + name)\n                torch.save(model.state_dict(), name)\n            else:\n                patient -= 1\n                log_and_print(logger, 'Decrease early-stopping patient by 1 due valid loss not decreasing. Patient='+ str(patient))\n\n            if(patient == 0):\n                log_and_print(logger, 'Early stopping patient = 0. Early stop')\n                break\n\n# ======================================================================\n\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-05T04:57:20.905005Z","iopub.execute_input":"2024-11-05T04:57:20.905614Z","iopub.status.idle":"2024-11-05T10:02:03.555728Z","shell.execute_reply.started":"2024-11-05T04:57:20.905571Z","shell.execute_reply":"2024-11-05T10:02:03.554011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model and Predict","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom segmentation_models_pytorch.unetplusplus.model import UnetPlusPlus\nimport cv2\n\n\n\ndef get_seg_model(candidate,weight_path):\n    model = UnetPlusPlus(\n        encoder_name = candidate['backbone_name'],\n        encoder_depth = 5,\n        encoder_weights = None,\n        classes = 2,\n        activation = 'sigmoid',\n    )\n#     print(weight_path)\n    \n    model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n        \n    return model\n\nSEG_MODEL = {\n        'backbone_name':'densenet121',\n        'pretranied_weight':'/kaggle/input/models-for-segementation/Fold0_densenet121_2d_segment (4).pth'\n\n    }\n\nseg_model = get_seg_model(SEG_MODEL,SEG_MODEL['pretranied_weight'])\n\n# print(SEG_MODEL['pretranied_weight'])\nseg_model.eval()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T06:46:52.159515Z","iopub.execute_input":"2024-11-11T06:46:52.160529Z","iopub.status.idle":"2024-11-11T06:46:54.440507Z","shell.execute_reply.started":"2024-11-11T06:46:52.160481Z","shell.execute_reply":"2024-11-11T06:46:54.439616Z"},"scrolled":true,"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1171723368.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"UnetPlusPlus(\n  (encoder): DenseNetEncoder(\n    (features): Sequential(\n      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu0): ReLU(inplace=True)\n      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (denseblock1): _DenseBlock(\n        (denselayer1): _DenseLayer(\n          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer2): _DenseLayer(\n          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer3): _DenseLayer(\n          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer4): _DenseLayer(\n          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer5): _DenseLayer(\n          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer6): _DenseLayer(\n          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (transition1): _Transition(\n        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n      )\n      (denseblock2): _DenseBlock(\n        (denselayer1): _DenseLayer(\n          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer2): _DenseLayer(\n          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer3): _DenseLayer(\n          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer4): _DenseLayer(\n          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer5): _DenseLayer(\n          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer6): _DenseLayer(\n          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer7): _DenseLayer(\n          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer8): _DenseLayer(\n          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer9): _DenseLayer(\n          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer10): _DenseLayer(\n          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer11): _DenseLayer(\n          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer12): _DenseLayer(\n          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (transition2): _Transition(\n        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n      )\n      (denseblock3): _DenseBlock(\n        (denselayer1): _DenseLayer(\n          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer2): _DenseLayer(\n          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer3): _DenseLayer(\n          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer4): _DenseLayer(\n          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer5): _DenseLayer(\n          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer6): _DenseLayer(\n          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer7): _DenseLayer(\n          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer8): _DenseLayer(\n          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer9): _DenseLayer(\n          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer10): _DenseLayer(\n          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer11): _DenseLayer(\n          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer12): _DenseLayer(\n          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer13): _DenseLayer(\n          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer14): _DenseLayer(\n          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer15): _DenseLayer(\n          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer16): _DenseLayer(\n          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer17): _DenseLayer(\n          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer18): _DenseLayer(\n          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer19): _DenseLayer(\n          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer20): _DenseLayer(\n          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer21): _DenseLayer(\n          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer22): _DenseLayer(\n          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer23): _DenseLayer(\n          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer24): _DenseLayer(\n          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (transition3): _Transition(\n        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n      )\n      (denseblock4): _DenseBlock(\n        (denselayer1): _DenseLayer(\n          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer2): _DenseLayer(\n          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer3): _DenseLayer(\n          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer4): _DenseLayer(\n          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer5): _DenseLayer(\n          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer6): _DenseLayer(\n          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer7): _DenseLayer(\n          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer8): _DenseLayer(\n          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer9): _DenseLayer(\n          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer10): _DenseLayer(\n          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer11): _DenseLayer(\n          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer12): _DenseLayer(\n          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer13): _DenseLayer(\n          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer14): _DenseLayer(\n          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer15): _DenseLayer(\n          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (denselayer16): _DenseLayer(\n          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu1): ReLU(inplace=True)\n          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu2): ReLU(inplace=True)\n          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (decoder): UnetPlusPlusDecoder(\n    (center): Identity()\n    (blocks): ModuleDict(\n      (x_0_0): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_1): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(1280, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_1_1): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(896, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_1_2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_2_2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_1_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_2_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_3_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_4): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n    )\n  )\n  (segmentation_head): SegmentationHead(\n    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Identity()\n    (2): Activation(\n      (activation): Sigmoid()\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# 3. Define the Preprocessing Pipeline\ntransform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Adjust size as required\n    transforms.ToTensor(),          # Convert to tensor\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize (if needed)\n])\n\n# 4. Load and Prepare the Input Data (e.g., Image)\nimage_path = \"/kaggle/input/processed-segementation/processed_segmentation_data/2D_slice_data/BraTS2021_00003/BraTS2021_00003_t1/BraTS2021_00003_t1_axial_000.npy\"\n\n\ndef preprocess_numpy_image(np_image):\n    \"\"\"Preprocess the NumPy image to match the model's input.\"\"\"\n    # Assume the input NumPy array has shape (H, W, C)\n    if np_image.ndim == 3:\n        np_image = np_image.transpose(2, 0, 1)  # Convert to (C, H, W)\n    elif np_image.ndim == 2:\n        np_image = np.expand_dims(np_image, axis=0)  # For grayscale (1, H, W)\n\n    # Convert to float32 and normalize (if needed)\n    np_image = np_image.astype(np.float32) / 255.0  # Scale to [0, 1]\n\n    # Normalize: Adjust values based on ImageNet stats (for DenseNet)\n    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n    np_image = (np_image - mean) / std\n\n    # Convert to tensor and add batch dimension\n    tensor_image = torch.tensor(np_image, dtype=torch.float32).unsqueeze(0)  # Shape: (1, C, H, W)\n    print(tensor_image.shape)\n    return tensor_image\n\n# 4. Example NumPy Image\nimage=np.load(image_path)\n\nimage_resized = cv2.resize(image, (128,128))\n\ninput_tensor = preprocess_numpy_image(image_resized)\n\n\n# 5. Make Predictions\n# with torch.no_grad():\noutput = seg_model(input_tensor)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T10:10:43.296527Z","iopub.execute_input":"2024-11-05T10:10:43.297206Z","iopub.status.idle":"2024-11-05T10:10:43.589194Z","shell.execute_reply.started":"2024-11-05T10:10:43.297165Z","shell.execute_reply":"2024-11-05T10:10:43.588125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicted = torch.argmax(output, dim=1).squeeze(0).numpy()  # Shape: (H, W)\n# print(f\"Predicted Mask Shape: {predicted.shape}\")\n# plt.imshow(predicted, cmap=\"gray\")\nimg=np.load(\"/kaggle/input/processed-segementation/processed_segmentation_data/2D_slice_data/BraTS2021_00003/BraTS2021_00003_t1/BraTS2021_00003_t1_axial_000.npy\")\nplt.imshow(img)\nplt.title(\"Predicted Mask\")\n# plt.figure()\n\nplt.imshow(output[0,1,:,:].detach().numpy(), cmap=\"gray\", alpha=0.45)\nplt.imshow(output[0,0,:,:].detach().numpy(), cmap=\"jet\", alpha=0.15)\nplt.figure()\n\nmask=np.load(\"/kaggle/input/processed-segementation/processed_segmentation_data/2D_slice_data/BraTS2021_00003/BraTS2021_00003_segmask/BraTS2021_00003_segmask_axial_000.npy\")\n# mask.resize(256,256)\n# print(mask.shape)\n\nplt.figure()\nplt.imshow(img)\nplt.imshow(mask[:,:], cmap=\"gray\", alpha=0.5)\nplt.title(\"Ground Truth\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T07:13:31.696926Z","iopub.execute_input":"2024-11-11T07:13:31.697469Z","iopub.status.idle":"2024-11-11T07:13:32.031265Z","shell.execute_reply.started":"2024-11-11T07:13:31.697420Z","shell.execute_reply":"2024-11-11T07:13:32.029975Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# predicted = torch.argmax(output, dim=1).squeeze(0).numpy()  # Shape: (H, W)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(f\"Predicted Mask Shape: {predicted.shape}\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plt.imshow(predicted, cmap=\"gray\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/processed-segementation/processed_segmentation_data/2D_slice_data/BraTS2021_00003/BraTS2021_00003_t1/BraTS2021_00003_t1_axial_000.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# import numpy as np\n# import matplotlib.pyplot as plt\n# import cv2\n# import torch\n# import json\n# import os\n# import shutil\n# from tqdm import tqdm\n\n# import pydicom\n# import glob\n# import sys\n# import argparse\n# import time\n\n# from multiprocessing import Pool\n\n# from segmentation_models_pytorch.unetplusplus.model import UnetPlusPlus\n# from segmentation_models_pytorch.losses import DiceLoss\n# from segmentation_models_pytorch.utils.metrics import IoU\n\n\n# argv = sys.argv[1:]  # Exclude the first argument (script name)\n# if '-f' in argv:\n#     argv = argv[:argv.index('-f')]  # Ignore everything after '-f'\n\n# # Set up the parser\n# parser = argparse.ArgumentParser(description='Insert some arguments')\n# parser.add_argument('--gpu', type=int, help='GPU ID', default=0)\n# parser.add_argument('--batch_size', type=int, help='Batch size', default=128)\n# parser.add_argument('--n_workers', type=int, help='Number of parallel workers', default=8)\n\n# # Parse the cleaned arguments\n# args = parser.parse_args(argv)\n\n\n# # with open('SETTINGS.json', 'r') as f:\n# SETTINGS = {\n#     \"DICOM_DATA_DIR\":\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification\",\n#     \"TASK1_DIR\":\"/kaggle/working/TrainingData\", \n#     \"CLASSIFICATION_RAW_JPG\":\"/kaggle/input/miccaibraintumorjpgdata/data\",\n#     \"SEGMENT_DATA_DIR\":\"/kaggle/input/segmentaed-csv\", \n#     \"CLASSIFICATION_DATA_DIR\":\"/kaggle/working/classification/data/\",\n#     \"KFOLD_PATH\":\"/kaggle/input/sub-file-for-tumor/data/train_stratifiedgroupkfold.csv\", \n#     \"SEGMENT_MODEL_DIR\":\"/kaggle/working/models/densenet121_2d_segment\",\n#     \"CLASSIFICATION_MODEL_DIR\":\"models/eca_nfnet_l0_2d_classification\",\n#     \"TEMP_DATA_DIR\":\"temp\",\n#     \"TEST_PREDICTION_FILE\":\"data/test_prediction.csv\"\n#         }\n\n# IM_FOLDER = SETTINGS['CLASSIFICATION_RAW_JPG']\n# OUT_FOLDER = SETTINGS['CLASSIFICATION_DATA_DIR']\n# SEGMENT_MODEL_DIR = '/kaggle/input/models-for-segementation/Fold0_densenet121_2d_segment (4).pth'\n\n# DEVICE = torch.device(f'cuda:{args.gpu}')\n\n# MRI_TYPES = ['T1w']\n\n# DIM = (224,224,3)\n# SEG_BATCH_SIZE = 32\n# N_WORKERS = 4\n\n# CANDIDATES = [\n#     {\n#         'backbone_name':'densenet121',\n# 'model_path':f'{SEGMENT_MODEL_DIR}'\n#     },\n# ]\n\n# # =============== Some helper functions ================\n\n# def get_model(candidate):\n#     model = UnetPlusPlus(\n#         encoder_name = candidate['backbone_name'],\n#         encoder_depth = 5,\n#         encoder_weights = None,\n#         classes = 2,\n#         activation = 'sigmoid',\n#     )\n\n#     weight_path = candidate.get('pretrained_weight')\n#     if(weight_path is not None):\n#         model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n        \n#     return model\n\n# import albumentations as A\n# from albumentations.pytorch.transforms import ToTensorV2\n\n# def get_transform(candidate, spatial_only=False):\n#     dim = candidate.get('dim', DIM)\n#     list_trans = [\n#                 A.Resize(width=int(dim[1]*1.2), height=int(dim[0]*1.2), always_apply=True),\n#                 A.CenterCrop(width=dim[1], height=dim[0], always_apply=True),\n#                 A.Normalize(), \n#                 ToTensorV2(p=1.0)\n#     ]\n#     return A.Compose(list_trans)\n\n# def get_inv_transform(original_w, original_h, candidate):\n#     dim = candidate.get('dim', DIM)\n#     list_trans = [\n#                 A.PadIfNeeded(min_height=int(dim[1]*1.2), min_width=int(dim[1]*1.2), always_apply=True),\n#                 A.Resize(width=original_w, height=original_h, always_apply=True),\n#     ]\n#     return A.Compose(list_trans)\n\n# def normalize_voxels(voxels):\n#     _min = voxels.min()\n#     _max = voxels.max()\n#     new_voxels = (voxels - _min) / (_max-_min) * 255.0\n#     return new_voxels\n\n# def check_empty(img, min_avg=0.1):\n#     _mean = np.where(img>0, 1, 0).mean()\n#     if(_mean > min_avg):\n#         return True\n#     return False\n\n\n# def find_largest_countours(contours):\n#     max_cnt = max(contours, key=lambda cnt: cv2.contourArea(cnt))\n#     return max_cnt\n\n# def has_good_features(image, mask, area_mask_over_image_min_ratio=0.1, max_count_mask_contours=5):\n#     _, image_thresh = cv2.threshold(image,1,255,cv2.THRESH_BINARY)\n#     image_contours, _ = cv2.findContours(image=image_thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n#     max_image_cnt = find_largest_countours(image_contours)\n    \n#     _, mask_thresh = cv2.threshold(mask,0.5,1,cv2.THRESH_BINARY)\n#     mask_contours, _ = cv2.findContours(image=mask_thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n#     count_n_mask_contours = len(mask_contours)\n#     if(count_n_mask_contours == 0):\n#         return False\n#     max_mask_cnt = find_largest_countours(mask_contours)\n    \n#     area_mask_over_image_ratio = cv2.contourArea(max_mask_cnt) / cv2.contourArea(max_image_cnt)\n    \n#     if(area_mask_over_image_ratio > area_mask_over_image_min_ratio \\\n#        and count_n_mask_contours <= max_count_mask_contours):\n#         return True\n#     else:\n#         return False\n    \n# def batch_predict_mask(data_loader, model):\n#     batch_out = []\n#     for batch_input in data_loader:\n#         batch_input = batch_input.to(DEVICE)\n\n#         batch_out.append(model(batch_input).cpu().detach().numpy())\n        \n#     batch_out = np.concatenate(batch_out, axis=0)\n#     batch_out = (batch_out > 0.5).astype('uint8')\n    \n#     del batch_input\n#     torch.cuda.empty_cache()\n    \n#     return batch_out\n\n\n# class BrainSegmentationInferDataset(torch.utils.data.Dataset):\n    \n#     def __init__(self, all_mri_voxels, transforms):\n#         self.all_mri_voxels = all_mri_voxels\n#         self.augmentations = transforms\n\n#     def __len__(self):\n#         return len(self.all_mri_voxels)\n\n#     def __getitem__(self, index):\n#         image = self.all_mri_voxels[index]\n#         image = np.stack([image]*3, axis=-1)\n        \n#         if self.augmentations:\n#             augmented = self.augmentations(image=image)\n#             image = augmented['image']\n\n#         return image\n\n# def error(e):\n#     print(e)\n    \n# def read_and_preprocess_voxels_update(args):\n#     if(args!=[]):\n#         voxels, mri_type, images = args\n#         global all_transformed_images, corresponding_mri_types, all_images\n#         all_transformed_images += [image for image in voxels]\n#         corresponding_mri_types += [mri_type]*len(voxels)\n#         all_images += images\n\n# def read_and_preprocess_voxels(patient_id, mri_type):\n#     paths = glob.glob(os.path.join(IM_FOLDER, patient_id, mri_type, '*.jpg'))\n#     paths = sorted(paths, key=lambda x: int(x.replace('.jpg','').split(\"-\")[-1]))\n#     positions = []\n#     images = []\n\n#     for path in paths:\n#     #     print(path)\n#     #     img = pydicom.dcmread(str(dcm_path))\n#     #     img = img.pixel_array\n#         img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n#         if(check_empty(img)):\n#             images.append(img)\n\n#     if(len(images) == 0):\n#         print(\"Found no images in case (patient_id, mri, path):\", patient_id, mri_type, paths)\n#         return []\n\n#     voxels = np.array(images)\n#     voxels = normalize_voxels(voxels)  # normalize voxels to range(0,255)\n# #     print(len(voxels))\n#     return voxels, mri_type, images\n        \n    \n# def sampling_one_image(patient_id, slice_index, image, out, mri_type):\n\n#     mask_0, mask_1 = out[0], out[1]\n#     inv_transforms = get_inv_transform(image.shape[1], image.shape[0], candidate)\n#     mask_0_original_size = inv_transforms(image=mask_0)['image']\n#     mask_1_original_size = inv_transforms(image=mask_1)['image']\n\n#     current_image_has_good_features = has_good_features(image, mask_0_original_size, \n#                                                         area_mask_over_image_min_ratio=0.025)\n\n#     if(not current_image_has_good_features):\n#         return None\n\n#     file_path = os.path.join(OUT_FOLDER + '/2D_slice_data/', \n#                                  f'BraTS2021_{patient_id}',\n#                                  f'BraTS2021_{patient_id}_{mri_type}',\n#                                 f'BraTS2021_{patient_id}_{mri_type}_{slice_index:03d}')\n#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n#     mask_0_original_size *= 255  # convert to 255 scale\n#     mask_1_original_size *= 255\n#     _3channel_data = np.stack([image, mask_0_original_size, mask_1_original_size], axis=-1)\n\n#     np.save(file_path, _3channel_data)\n\n#     return int(patient_id), mri_type, slice_index, file_path+'.npy'\n\n\n# def sampling_one_image_update(args):\n#     global list_patient_id, list_mri_type, list_slice_index, list_file_path\n#     if(args is not None):\n#         patient_id, mri_type, slice_index, file_path = args\n#         list_patient_id.append(patient_id)\n#         list_mri_type.append(mri_type)\n#         list_slice_index.append(slice_index)\n#         list_file_path.append(file_path)\n# # =======================================================        \n        \n    \n# # =============== Generate masks combined with image ==============\n\n# #     shutil.rmtree(OUT_FOLDER)  # REMOVE EXISTING DIR. BECARE FULL USING THIS        \n               \n# candidate = CANDIDATES[0]\n# model = get_model(candidate)\n# model.load_state_dict(torch.load(candidate['model_path'], map_location='cpu'))\n# model.to(DEVICE)\n\n# model.eval()\n# print()        \n        \n# list_patient_id = []\n# list_slice_index = []\n# list_mri_type = []\n# list_file_path = []\n\n# os.makedirs(OUT_FOLDER, exist_ok=True)\n# for pi, patient_id in tqdm(enumerate(os.listdir(IM_FOLDER))):\n#     fol=\"BraTS2021_\"+patient_id\n#     if(os.path.exists(os.path.join(OUT_FOLDER,\"2D_slice_data\",fol))):\n#         continue\n#     folders=os.listdir(os.path.join(IM_FOLDER,patient_id))\n# #     print(folders)\n#     if \"T1w\" not in folders:\n#        continue\n#     s1 = time.time()\n    \n#     all_transformed_images = []\n#     corresponding_mri_types = []\n#     all_images = []\n    \n#     pool = Pool(processes=N_WORKERS)   \n\n#     for mri_type in MRI_TYPES:\n#         pool.apply_async(\n#             read_and_preprocess_voxels,\n#             args=(patient_id, mri_type),\n#             callback=read_and_preprocess_voxels_update,\n#             error_callback=error,\n#         )\n\n#     pool.close()\n#     pool.join()    \n            \n#     e1 = time.time()\n    \n#     s2 = time.time()\n    \n    \n    \n    \n    \n    \n    \n    \n#     transform = get_transform(candidate)  # transform for segmentation input\n#     seg_infer_ds = BrainSegmentationInferDataset(all_transformed_images, transform)\n#     seg_infer_loader = torch.utils.data.DataLoader(seg_infer_ds, batch_size=SEG_BATCH_SIZE, shuffle=False,\n#                         num_workers=N_WORKERS, pin_memory=torch.cuda.is_available())\n   \n#     batch_out = batch_predict_mask(seg_infer_loader, model)\n    \n#     e2 = time.time()\n    \n#     s3 = time.time()\n\n#     # sampling slices by mask area\n#     pool = Pool(processes=N_WORKERS)   \n    \n#     for i in range(len(all_images)):\n#         image = all_images[i]\n#         out = batch_out[i]\n#         mri_type = corresponding_mri_types[i]\n        \n#         pool.apply_async(\n#             sampling_one_image,\n#             args=(patient_id, i, image, out, mri_type),\n#             callback=sampling_one_image_update,\n#             error_callback=error,\n#         )\n        \n#     pool.close()\n#     pool.join()   \n    \n#     del batch_out\n#     torch.cuda.empty_cache()\n        \n#     e3 = time.time()\n\n# #     print(f'Patial time: read time: {e1-s1}. mask pred time: {e2-s2}. sampling time: {e3-s3}')\n        \n# out_df = pd.DataFrame({\n#     'BraTS21ID':list_patient_id,\n#     'mri_type':list_mri_type,\n#     'slice_index':list_slice_index,\n#     'file_path':list_file_path,\n# })\n\n# out_df.to_csv(os.path.join(OUT_FOLDER, 'meta_classification.csv'), index=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-29T10:41:02.701909Z","iopub.execute_input":"2024-10-29T10:41:02.702251Z","iopub.status.idle":"2024-10-29T10:41:02.720032Z","shell.execute_reply.started":"2024-10-29T10:41:02.702218Z","shell.execute_reply":"2024-10-29T10:41:02.719166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n\n# # Path to the folder you want to zip (inside your Kaggle environment)\n# folder_to_zip = \"/kaggle/working/classification/data\"  # Replace 'my_folder' with your folder name\n\n# # Output path for the zip file\n# zip_file_path = \"/kaggle/working/my_folder.zip\"\n\n# # Create a zip archive of the folder\n# shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_to_zip)\n\n# print(f\"Folder zipped successfully at: {zip_file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:41:02.721114Z","iopub.execute_input":"2024-10-29T10:41:02.721399Z","iopub.status.idle":"2024-10-29T10:41:02.732107Z","shell.execute_reply.started":"2024-10-29T10:41:02.721368Z","shell.execute_reply":"2024-10-29T10:41:02.731187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport logging\nimport pandas as pd \nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import nn\n\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CosineAnnealingLR\n\nimport torch.nn.functional as F\n\nfrom segmentation_models_pytorch.unetplusplus.model import UnetPlusPlus\nfrom segmentation_models_pytorch.losses import DiceLoss\nfrom segmentation_models_pytorch.utils.metrics import IoU\n\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom torch import nn\nimport gc\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport json\nimport argparse\n\nargv = sys.argv[1:]  # Exclude the first argument (script name)\nif '-f' in argv:\n    argv = argv[:argv.index('-f')]  # Ignore everything after '-f'\n\n# Set up the parser\nparser = argparse.ArgumentParser(description='Insert some arguments')\nparser.add_argument('--gpu', type=int, help='GPU ID', default=0)\nparser.add_argument('--batch_size', type=int, help='Batch size', default=128)\nparser.add_argument('--n_workers', type=int, help='Number of parallel workers', default=8)\n\n# Parse the cleaned arguments\nargs = parser.parse_args(argv)\n\n\n# with open('SETTINGS.json', 'r') as f:\nSETTINGS = {\n    \"DICOM_DATA_DIR\":\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification\",\n    \"TASK1_DIR\":\"/kaggle/working/TrainingData\", \n    \"CLASSIFICATION_RAW_JPG\":\"/kaggle/input/miccaibraintumorjpgdata/data\",\n    \"SEGMENT_DATA_DIR\":\"/kaggle/input/segmentaed-csv\", \n    \"CLASSIFICATION_DATA_DIR\":\"/kaggle/working/classification/data/\",\n    \"KFOLD_PATH\":\"/kaggle/input/sub-file-for-tumor/data/train_stratifiedgroupkfold.csv\", \n    \"SEGMENT_MODEL_DIR\":\"/kaggle/working/models/densenet121_2d_segment\",\n    \"CLASSIFICATION_MODEL_DIR\":\"models/eca_nfnet_l0_2d_classification\",\n    \"TEMP_DATA_DIR\":\"temp\",\n    \"TEST_PREDICTION_FILE\":\"data/test_prediction.csv\"\n        }\n\nDATA_FOLDER = SETTINGS['CLASSIFICATION_DATA_DIR']\nMETA_FILE_PATH = '/kaggle/input/pre-processeddata-class/metalfile_classification.csv'\nKFOLD_FILE_PATH = SETTINGS['KFOLD_PATH']\n\nRUN_FOLDS = [0]\nMRI_TYPES = ['T1w', ]\nSTRIDE = 5\nSEQ_LEN = 35\nLSTM_HIDDEN_SIZE = 128\nLSTM_LAYERS = 1\nSEED = 67\nDIM = (224, 224, 3)\nN_WORKERS = 4\nBATCH_SIZE = 8\nBASE_LR = 1e-3\nNUM_EPOCHS = 10\nPATIENT = 10\nSAMPLE = None\nDEVICE = torch.device(f'cuda:{args.gpu}')\n\nPARENT_OUT_FOLDER = 'models/'   \n\nCANDIDATES = [\n    {\n        'backbone_name':'eca_nfnet_l0',\n        'ver_note':'2d_classification',\n        'backbone_pretrained':None,\n        'batch_size':BATCH_SIZE,\n        'warm_up_epochs':5,\n    },\n]\n\n\nimport sys\n# from utils.general import seed_torch, init_progress_dict, log_to_progress_dict, save_progress, log_and_print, get_logger\n\n# seed every thing\nseed_torch(SEED)\n\n\ndef chunk_slices(list_files):\n    list_files = sorted(list_files)\n    chunks = []\n    n_chunks = max(int(np.ceil((len(list_files) - SEQ_LEN) / STRIDE ) + 1),1)\n    for i in range(n_chunks):\n        s = i*STRIDE\n        e = min(s+SEQ_LEN, len(list_files))\n        chunks.append(list_files[s:e])\n    return chunks\n\ndef expand(row):\n    list_files = row['chunk_file_paths']\n    return pd.DataFrame({\n        'BraTS21ID':[row['BraTS21ID']]*len(list_files),\n        'MGMT_value':[row['MGMT_value']]*len(list_files),\n        'mri_type':[row['mri_type']]*len(list_files),\n        'file_path':list_files,\n        'fold':[row['fold']]*len(list_files)\n    })\n\ndef get_first_value(df, col_name):\n    df[col_name] = df[col_name].map(lambda x: list(x)[0])\n\n    \ndef process_df_mri_type(df_mri):\n    df_mri_group = df_mri.groupby('BraTS21ID').agg(list)\n    df_mri_group = df_mri_group.reset_index()\n    df_mri_group['chunk_file_paths'] = df_mri_group.file_path.map(chunk_slices)\n    df_mri_group['chunk_count'] = df_mri_group['chunk_file_paths'].map(lambda x: len(x))\n    df_mri_group['chunk_cum_count'] = df_mri_group['chunk_count'].cumsum()\n    df_mri_group_expand = df_mri_group.apply(expand, axis=1).tolist()\n    df_mri_group_expand = pd.concat(df_mri_group_expand)\n\n    for col_name in ['MGMT_value', 'mri_type', 'fold']:\n        get_first_value(df_mri_group_expand, col_name)\n        \n    return df_mri_group_expand    \n    \nclass BrainClassification2DDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, csv, transforms=None):\n        self.csv = csv.reset_index(drop=True)\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        list_file_path = row['file_path']\n        list_images = []\n        label = row['MGMT_value']\n        for i, path in enumerate(list_file_path):\n            image = np.load(path)\n            label = row['MGMT_value']\n            list_images.append(image)\n                \n        images = np.stack(list_images, axis=0)\n        if(images.shape[0] < SEQ_LEN):\n            n_pad = SEQ_LEN - images.shape[0]\n            pad_matrix = np.zeros(shape=(n_pad, images.shape[1], images.shape[2], images.shape[3]))\n            images = np.concatenate([images, pad_matrix], axis=0)\n            \n        if self.augmentations:\n            images_dict = dict()\n            for i in range(len(images)):\n                if(i==0):\n                    images_dict['image'] = images[i]\n                else:\n                    images_dict[f'image{i-1}'] = images[i]\n            augmented = self.augmentations(**images_dict)\n            \n            transformed_images = []\n            for i in range(len(images)):\n                if(i==0):\n                    transformed_images.append(augmented['image'])\n                else:\n                    transformed_images.append(augmented[f'image{i-1}'])\n                    \n            transformed_images = np.stack(transformed_images, axis=0)\n            return transformed_images, torch.tensor(label)\n            \n        return images, torch.tensor(label)\n    \nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\ndef get_train_transforms(candidate):\n    dim = candidate.get('dim', DIM)\n    seq_len = candidate.get('seq_len', SEQ_LEN)\n    additional_targets = {f'image{i}':'image' for i in range(SEQ_LEN-1)}\n    return A.Compose(\n        [\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(p=0.5),\n            \n            A.Resize(width=dim[1], height=dim[0], always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ],\n        additional_targets=additional_targets\n    )\n\ndef get_valid_transforms(candidate):\n    dim = candidate.get('dim', DIM)\n    additional_targets = {f'image{i}':'image' for i in range(SEQ_LEN-1)}\n    return A.Compose(\n        [\n            A.Resize(width=dim[1], height=dim[0], always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ],\n        additional_targets=additional_targets\n    )    \n\ndef dfs_freeze(module):\n    for name, child in module.named_children():\n        for param in child.parameters():\n            param.requires_grad = False\n        dfs_freeze(child)\n        \ndef dfs_unfreeze(module):\n    for name, child in module.named_children():\n        for param in child.parameters():\n            param.requires_grad = True\n        dfs_unfreeze(child)\n\nimport timm\n\nclass BrainSequenceModelNFNet(nn.Module):\n    def __init__(self, backbone_name, backbone_pretrained,\n                 lstm_dim=64, lstm_layers=1, lstm_dropout=0., \n                 n_classes=1):\n        super(BrainSequenceModelNFNet, self).__init__()\n        self.backbone = timm.create_model(backbone_name, pretrained=False)\n#         self.backbone.load_state_dict(torch.load(backbone_pretrained))\n        \n        lstm_inp_dim = self.backbone.head.fc.in_features\n        \n        self.backbone.head.fc = nn.Identity()\n        \n        self.lstm = nn.LSTM(lstm_inp_dim, lstm_dim, num_layers=lstm_layers, \n                            batch_first=True, bidirectional=True,\n                            dropout=lstm_dropout)\n        \n        self.clf_head = nn.Linear(lstm_dim*2*SEQ_LEN, n_classes)\n        \n    def forward(self, x):\n        n = x.shape[0]\n        seq_length = x.shape[1]\n        concat_x = torch.cat([x[i] for i in range(n)], axis=0)\n        concat_x = self.backbone(concat_x)\n        \n        \n        stacked_x = torch.stack([concat_x[i*seq_length:i*seq_length+seq_length] for i in range(n)], axis=0)\n        \n        seq_features, _ = self.lstm(stacked_x)\n        seq_features = seq_features.reshape(n,-1)\n        \n        logits = self.clf_head(seq_features)\n        \n        return logits\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef train_valid_fn(dataloader,model, criterion, scaler, optimizer=None,device='cuda:0',scheduler=None,\n                   epoch=0,mode='train', metric='auc'):\n    '''Perform model training'''\n    if(mode=='train'):\n        model.train()\n    elif(mode=='valid'):\n        model.eval()\n    else:\n        raise ValueError('No such mode')\n        \n    loss_score = AverageMeter()\n    \n    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n    all_predictions = []\n    all_labels = []\n    for i, batch in tk0:\n        if(mode=='train'):\n            optimizer.zero_grad()\n            \n        # input, gt\n        voxels, labels = batch\n        voxels = voxels.to(device)\n        labels = labels.to(device).float()\n\n        # prediction\n        with torch.cuda.amp.autocast():\n            logits = model(voxels)\n            logits = logits.view(-1)\n            probs = logits.sigmoid()\n            # compute loss\n            loss = criterion(logits, labels)\n        \n        if(mode=='train'):\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        \n        loss_score.update(loss.detach().cpu().item(), dataloader.batch_size)\n\n        # append for metric calculation\n        all_predictions.append(probs.detach().cpu().numpy())\n        all_labels.append(labels.detach().cpu().numpy())\n        \n        if(mode=='train'):\n            tk0.set_postfix(Loss_Train=loss_score.avg, Epoch=epoch, LR=optimizer.param_groups[0]['lr'])\n        elif(mode=='valid'):\n            tk0.set_postfix(Loss_Valid=loss_score.avg, Epoch=epoch)\n        \n        del batch, voxels, labels, logits, probs, loss\n        torch.cuda.empty_cache()\n\n    if(mode=='train'):\n        if(scheduler.__class__.__name__ == 'CosineAnnealingWarmRestarts'):\n            scheduler.step(epoch=epoch)\n        elif(scheduler.__class__.__name__ == 'ReduceLROnPlateau'):\n            scheduler.step(loss_score.avg)\n\n    all_predictions = np.concatenate(all_predictions)\n    all_labels = np.concatenate(all_labels)\n    if(metric == 'auc'):\n        auc = roc_auc_score(y_true=all_labels, y_score=all_predictions)\n        return loss_score.avg, auc \n    \n    return loss_score.avg\n\n    \n# ============ Read metadata ==============    \ndf = pd.read_csv(META_FILE_PATH)\nkfold_df = pd.read_csv(KFOLD_FILE_PATH)\ndf = df.merge(kfold_df, on='BraTS21ID')\n\ndf_flair = df[df.mri_type=='FLAIR']\ndf_t1 = df[df.mri_type=='T1w']\ndf_t1ce = df[df.mri_type=='T1wCE']\ndf_t2 = df[df.mri_type=='T2w']\n# =========================================\n\n\n# ================================ Training ==================================\nfor candidate in CANDIDATES:\n    print(f\"######################### Candidate: {candidate['backbone_name']} ############################\")\n    run_folds = candidate.get('run_folds', RUN_FOLDS)\n    \n    parent_out_folder = candidate.get('parent_out_folder', PARENT_OUT_FOLDER)\n    ver_note = candidate['ver_note']\n\n    for mri_type in MRI_TYPES:\n        out_folder_name = f\"{candidate['backbone_name']}_{ver_note}\"\n        out_folder = os.path.join(parent_out_folder, out_folder_name, mri_type)\n        os.makedirs(out_folder, exist_ok=True)\n    \n        for valid_fold in run_folds:\n            # Read data\n            if(SAMPLE):\n                df = df.sample(SAMPLE, random_state=SEED)\n            if(mri_type != 'all'):\n                df_mri = df[df.mri_type==mri_type]\n            \n            # process data\n            df_mri = process_df_mri_type(df_mri)\n                \n            train_df = df_mri[df_mri.fold!=valid_fold]\n            valid_df = df_mri[df_mri.fold==valid_fold]\n\n            print(f'\\n\\n================= Fold {valid_fold}. MRI: {mri_type} ==================')\n            print(f'Number of training samples: {len(train_df)}. Number of valid samples: {len(valid_df)}')\n\n            # train and valid transforms\n            train_transforms = get_train_transforms(candidate)\n            valid_transforms = get_valid_transforms(candidate)\n\n            # create data loader\n            train_dataset =  BrainClassification2DDataset(train_df, train_transforms)\n            valid_dataset = BrainClassification2DDataset(valid_df, valid_transforms)\n\n            batch_size = candidate.get('batch_size', BATCH_SIZE)\n            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                            num_workers=N_WORKERS, pin_memory=torch.cuda.is_available())\n            valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n                            num_workers=N_WORKERS, pin_memory=torch.cuda.is_available())\n\n\n            # Model\n            model = BrainSequenceModelNFNet(candidate['backbone_name'], \n                                            candidate['backbone_pretrained'],\n                                           lstm_dim=LSTM_HIDDEN_SIZE,lstm_layers=LSTM_LAYERS)\n            model.to(DEVICE)\n            print()\n\n            warm_start_weight = candidate.get('warm_start_weight')\n            if(warm_start_weight):\n                print('Load warm start weight:', warm_start_weight)\n\n            # freeze pretrained layers\n#             dfs_freeze(model.backbone)\n#             print(' -------- Start warm up process ----------')\n#             print('Freeze backbone')\n#             model = model.to(DEVICE)\n#             print()\n\n\n            # Optimizer and scheduler\n            base_lr = candidate.get('base_lr', BASE_LR)\n            optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=BASE_LR)\n\n            num_training_steps = NUM_EPOCHS * len(train_loader)\n            lr_scheduler = ReduceLROnPlateau(optimizer=optim, factor=0.67, patience=3, verbose=True)\n\n            # loss\n            criterion = nn.BCEWithLogitsLoss()\n\n\n            # use amp to accelerate training\n            scaler = torch.cuda.amp.GradScaler()\n\n            # Logging\n            logger = get_logger(\n                name = f'training_log_fold{valid_fold}.txt',\n                path=os.path.join(out_folder, f'training_log_fold{valid_fold}.txt')\n            )\n\n            best_valid_loss = 9999\n            best_valid_ep = 0\n            patient = PATIENT\n\n            progress_dict = init_progress_dict(['loss', 'AUC'])\n\n            start_ep = candidate.get('warm_start_ep', 1)\n            print('Start ep:', start_ep)\n\n            # warm up epochs\n            warm_up_epochs = candidate.get('warm_up_epochs', 0)\n\n\n            for epoch in range(start_ep, NUM_EPOCHS+1):\n                if(epoch==warm_up_epochs+1):\n                    print(' -------- Finish warm up process ----------')\n                    print('Unfreeze backbone')\n                    dfs_unfreeze(model.backbone)\n                    optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=BASE_LR)\n                    lr_scheduler = ReduceLROnPlateau(optimizer=optim)\n\n                # =============== Training ==============\n                train_loss, train_auc = train_valid_fn(train_loader,model,criterion, scaler, optimizer=optim,device=DEVICE,\n                                            scheduler=lr_scheduler,epoch=epoch,mode='train', metric='auc')\n                valid_loss, valid_auc = train_valid_fn(valid_loader,model,criterion, scaler, device=DEVICE,epoch=epoch,mode='valid', metric='auc')\n\n                current_lr = optim.param_groups[0]['lr']\n                log_line = f'Model: {out_folder_name}. Epoch: {epoch}. '\n                log_line += f'Train loss:{train_loss} - Valid loss: {valid_loss}. '\n                log_line += f'Train AUC:{train_auc} - Valid AUC: {valid_auc}. '\n                log_line += f'Lr: {current_lr}.'\n\n                log_and_print(logger, log_line)\n\n                metric_dict = {'train_loss':train_loss,'valid_loss':valid_loss,\n                               'train_AUC':train_auc, 'valid_AUC':valid_auc,\n                           }\n\n                progress_dict = log_to_progress_dict(progress_dict, metric_dict)\n\n                # plot figure and save the progress chart\n                save_progress(progress_dict, out_folder, out_folder_name, valid_fold, show=False)\n\n                if(valid_loss < best_valid_loss):\n                    best_valid_loss = valid_loss\n                    best_valid_ep = epoch\n                    patient = PATIENT # reset patient\n\n                    # save model\n                    name = os.path.join(out_folder, f'%s_Fold%d_%s.pth'%(mri_type, valid_fold, out_folder_name))\n                    log_and_print(logger, 'Saving model to: ' + name)\n                    torch.save(model.state_dict(), name)\n                else:\n                    patient -= 1\n                    log_and_print(logger, 'Decrease early-stopping patient by 1 due valid loss not decreasing. Patient='+ str(patient))\n\n                if(patient == 0):\n                    log_and_print(logger, 'Early stopping patient = 0. Early stop')\n                    break\n# =============================================================================","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:41:02.733735Z","iopub.execute_input":"2024-10-29T10:41:02.734358Z","iopub.status.idle":"2024-10-29T10:41:09.581753Z","shell.execute_reply.started":"2024-10-29T10:41:02.734325Z","shell.execute_reply":"2024-10-29T10:41:09.576639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img=np.load(\"/kaggle/working/classification/data/2D_slice_data/BraTS2021_00185/BraTS2021_00185_T1w/BraTS2021_00185_T1w_030.npy\")\n# plt.imshow(img)\n# plt.figure()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:41:09.583930Z","iopub.status.idle":"2024-10-29T10:41:09.584785Z","shell.execute_reply.started":"2024-10-29T10:41:09.584490Z","shell.execute_reply":"2024-10-29T10:41:09.584535Z"},"trusted":true},"execution_count":null,"outputs":[]}]}